const axios = require("axios");

// Llama model seÃ§enekleri
const LLAMA_MODELS = {
    llama3: {
        name: "Llama 3",
        description: "En son Llama modeli - yÃ¼ksek kalite",
        ram: "8GB",
        speed: "HÄ±zlÄ±",
    },
    "llama3.2:3b": {
        name: "Llama 3.2 3B",
        description: "HÄ±zlÄ± ve hafif, genel kullanÄ±m iÃ§in ideal",
        ram: "2GB",
        speed: "Ã‡ok hÄ±zlÄ±",
    },
    "llama3.2:7b": {
        name: "Llama 3.2 7B",
        description: "Dengeli performans ve hÄ±z",
        ram: "4GB",
        speed: "HÄ±zlÄ±",
    },
    "llama3.2:70b": {
        name: "Llama 3.2 70B",
        description: "En yÃ¼ksek kalite (daha fazla RAM gerekli)",
        ram: "40GB",
        speed: "YavaÅŸ",
    },
    "phi3:mini": {
        name: "Phi-3 Mini",
        description: "Ã‡ok hÄ±zlÄ± ve hafif",
        ram: "1.5GB",
        speed: "Ã‡ok hÄ±zlÄ±",
    },
    "phi3:small": {
        name: "Phi-3 Small",
        description: "Daha iyi kalite, hala hÄ±zlÄ±",
        ram: "3GB",
        speed: "HÄ±zlÄ±",
    },
};

const AI_CONFIG = {
    baseURL: process.env.OLLAMA_URL || "http://localhost:11434/api",
    model: process.env.OLLAMA_CHAT_MODEL || "llama3.2:3b", // VarsayÄ±lan olarak llama3.2:3b model
    defaultParams: {
        temperature: parseFloat(process.env.AI_TEMPERATURE) || 0.7,
        max_tokens: parseInt(process.env.AI_MAX_TOKENS) || 512,
    },
};

// Model bilgilerini getir
const getModelInfo = (modelName) => {
    return (
        LLAMA_MODELS[modelName] || {
            name: modelName,
            description: "Bilinmeyen model",
            ram: "Bilinmiyor",
            speed: "Bilinmiyor",
        }
    );
};

// Mevcut modelleri listele
const listAvailableModels = async () => {
    try {
        const response = await axios.get(`${AI_CONFIG.baseURL}/tags`);
        return response.data.models || [];
    } catch (error) {
        console.error("âŒ Model listesi alÄ±namadÄ±:", error.message);
        return [];
    }
};

const queryAI = async (prompt) => {
    try {
        console.log(
            `ğŸ¤– Ollama API'ye istek gÃ¶nderiliyor... (Model: ${AI_CONFIG.model})`
        );

        const response = await axios.post(
            `${AI_CONFIG.baseURL}/generate`,
            {
                model: AI_CONFIG.model,
                prompt: prompt,
                temperature: AI_CONFIG.defaultParams.temperature,
                max_tokens: AI_CONFIG.defaultParams.max_tokens,
                stream: false,
            },
            {
                headers: {
                    "Content-Type": "application/json",
                },
                timeout: 300000, // 5 dakika timeout
            }
        );

        console.log("âœ… Ollama yanÄ±tÄ± alÄ±ndÄ±");

        return {
            choices: [
                {
                    text: response.data.response.trim(),
                },
            ],
        };
    } catch (error) {
        console.error("âŒ Ollama API hatasÄ±:", error.message);

        if (error.code === "ECONNREFUSED") {
            throw new Error(
                "Ollama servisi Ã§alÄ±ÅŸmÄ±yor. LÃ¼tfen 'ollama serve' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n."
            );
        }

        if (error.response?.status === 404) {
            throw new Error(
                `Model '${AI_CONFIG.model}' bulunamadÄ±. LÃ¼tfen 'ollama pull ${AI_CONFIG.model}' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.`
            );
        }

        if (
            error.code === "ECONNABORTED" ||
            error.message.includes("timeout")
        ) {
            throw new Error(
                "Ollama modeli yanÄ±t vermek iÃ§in Ã§ok uzun sÃ¼rdÃ¼. LÃ¼tfen daha sonra tekrar deneyin."
            );
        }

        throw new Error(`AI servis hatasÄ±: ${error.message}`);
    }
};

module.exports = {
    queryAI,
    AI_CONFIG,
    LLAMA_MODELS,
    getModelInfo,
    listAvailableModels,
};
