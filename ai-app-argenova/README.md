# AI App Argenova

Bu proje, modern ve kullanÄ±cÄ± dostu bir AI chat uygulamasÄ±dÄ±r. HaftalÄ±k Ã§alÄ±ÅŸma verilerini analiz eden bu asistan, Qdrant vektÃ¶r veritabanÄ± ve geliÅŸmiÅŸ RAG (retrieval-augmented generation) mimarisi ile geÃ§miÅŸ sorgularÄ± analiz eder ve daha eÄŸitilmiÅŸ, ÅŸeffaf yanÄ±tlar Ã¼retir.

## ğŸš€ **Yeni Ã–zellikler ve Modern Chat ArayÃ¼zÃ¼**

-   ChatGPT tarzÄ± baloncuklar, otomatik kaydÄ±rma, loading animasyonu
-   Sidebar'da geÃ§miÅŸ sorgular ve vektÃ¶r veritabanÄ± sekmeleri
-   Responsive ve mobil uyumlu, modern bir tasarÄ±m
-   YanÄ±t stili (detaylÄ±, teknik, sade), formatÄ± (zengin, madde, tablo, kod) ve uzunluÄŸu (kÄ±sa, detaylÄ±) seÃ§ilebilir
-   Her yanÄ±tÄ±n altÄ±nda, kullanÄ±lan benzer Ã¶rnekler (kaynaklar) ve otomatik deÄŸerlendirme (selfCheck) gÃ¶sterilir
-   KullanÄ±cÄ±, yanÄ±t iÃ§in "beÄŸendim/beÄŸenmedim" feedback'i verebilir
-   Sohbet geÃ§miÅŸinden veya admin panelden, kaliteli log'lar eÄŸitim Ã¶rneÄŸi olarak iÅŸaretlenebilir ve dÄ±ÅŸa aktarÄ±labilir
-   Kod bloklarÄ±, tablolar ve madde iÅŸaretleri iÃ§in Ã¶zel CSS ile zengin format desteÄŸi

## ğŸš€ **Yeni Ã–zellik: Llama Model DesteÄŸi**

Proje artÄ±k **Llama 3.2** modellerini desteklemektedir. KullanÄ±cÄ±lar farklÄ± Llama modelleri arasÄ±nda geÃ§iÅŸ yapabilir:

### ğŸ¤– **Desteklenen Modeller**

| Model             | Boyut | RAM   | HÄ±z    | Kalite | Ã–nerilen KullanÄ±m        |
| ----------------- | ----- | ----- | ------ | ------ | ------------------------ |
| **Llama 3.2 3B**  | 3B    | 2GB   | âš¡âš¡âš¡ | ğŸŸ¡     | HÄ±zlÄ± testler, dÃ¼ÅŸÃ¼k RAM |
| **Llama 3.2 7B**  | 7B    | 4GB   | âš¡âš¡   | ğŸŸ¢     | **VarsayÄ±lan - Dengeli** |
| **Llama 3.2 70B** | 70B   | 40GB  | âš¡     | ğŸ”´     | En yÃ¼ksek kalite         |
| **Phi-3 Mini**    | 3.8B  | 1.5GB | âš¡âš¡âš¡ | ğŸŸ¡     | Ã‡ok hÄ±zlÄ±                |
| **Phi-3 Small**   | 7B    | 3GB   | âš¡âš¡   | ğŸŸ¢     | HÄ±zlÄ± ve kaliteli        |

### ğŸ“¥ **Model Kurulumu**

```bash
# Ollama'ya model indirme
ollama pull llama3.2:3b    # HÄ±zlÄ± model
ollama pull llama3.2:7b    # VarsayÄ±lan model
ollama pull llama3.2:70b   # YÃ¼ksek kalite (40GB RAM gerekli)
ollama pull phi3:mini      # Ã‡ok hÄ±zlÄ±
ollama pull phi3:small     # HÄ±zlÄ± ve kaliteli

# Mevcut modelleri listele
ollama list

# Model bilgilerini gÃ¶rÃ¼ntÃ¼le
ollama show llama3.2:7b
```

### ğŸ›ï¸ **Model SeÃ§imi**

1. **Web ArayÃ¼zÃ¼**: Header'daki dropdown'dan model seÃ§in
2. **Environment Variable**: `.env` dosyasÄ±nda `OLLAMA_MODEL=llama3.2:7b`
3. **Otomatik Kaydetme**: SeÃ§ilen model localStorage'da saklanÄ±r

## ğŸ—ï¸ Proje Mimarisi

Proje MVC (Model-View-Controller) mimarisine uygun olarak dÃ¼zenlenmiÅŸtir ve modern frontend ile tam entegredir:

```
ai-app-argenova/
â”œâ”€â”€ config/           # YapÄ±landÄ±rma dosyalarÄ±
â”‚   â”œâ”€â”€ database.js   # MongoDB baÄŸlantÄ±sÄ±
â”‚   â”œâ”€â”€ ai.js         # AI servis yapÄ±landÄ±rmasÄ± (Llama destekli)
â”‚   â”œâ”€â”€ qdrant.js     # Qdrant vektÃ¶r veritabanÄ±
â”‚   â””â”€â”€ embedding.js  # OpenAI embedding servisi
â”œâ”€â”€ models/           # VeritabanÄ± modelleri
â”‚   â””â”€â”€ Log.js        # Log ÅŸemasÄ±
â”œâ”€â”€ controllers/      # Ä°ÅŸ mantÄ±ÄŸÄ±
â”‚   â””â”€â”€ aiController.js # AI iÅŸlemleri
â”œâ”€â”€ routes/           # API route'larÄ±
â”‚   â””â”€â”€ aiRoutes.js   # AI endpoint'leri (Model yÃ¶netimi dahil)
â”œâ”€â”€ middleware/       # Ara yazÄ±lÄ±mlar
â”‚   â”œâ”€â”€ errorHandler.js    # Hata yÃ¶netimi
â”‚   â””â”€â”€ requestLogger.js   # Ä°stek loglama
â”œâ”€â”€ public/           # Statik dosyalar
â”‚   â”œâ”€â”€ index.html    # Modern chat arayÃ¼zÃ¼
â”‚   â”œâ”€â”€ style.css     # Responsive ve zengin tasarÄ±m
â”‚   â””â”€â”€ script.js     # Vue.js tabanlÄ± chat mantÄ±ÄŸÄ±
â”œâ”€â”€ server.js         # Ana sunucu dosyasÄ±
â”œâ”€â”€ package.json
â”œâ”€â”€ docker-compose.yml # Docker servisleri
â””â”€â”€ .env.example      # Environment variables Ã¶rneÄŸi
```

## ğŸš€ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

### 1. **Ollama Kurulumu**

```bash
# Ubuntu/Debian
curl -fsSL https://ollama.ai/install.sh | sh

# Ollama servisini baÅŸlat
ollama serve

# Ä°lk modeli indir
ollama pull llama3.2:7b
```

### 2. **Docker Servislerini BaÅŸlatÄ±n**

```bash
docker-compose up -d
```

### 3. **BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¼kleyin**

```bash
npm install
```

### 4. **Environment Variables DosyasÄ±nÄ± OluÅŸturun**

```bash
cp .env.example .env
```

### 5. **.env DosyasÄ±nÄ± DÃ¼zenleyin**

```env
# AI Service Configuration
AI_SERVICE_URL=http://localhost:11434/api
AI_TEMPERATURE=0.7
AI_MAX_TOKENS=512

# Ollama Model Configuration
OLLAMA_MODEL=llama3.2:7b
# Model seÃ§enekleri:
# - llama3.2:3b (HÄ±zlÄ±, hafif - 2GB RAM)
# - llama3.2:7b (Dengeli - 4GB RAM)
# - llama3.2:70b (En yÃ¼ksek kalite - 40GB RAM)
# - phi3:mini (Ã‡ok hÄ±zlÄ± - 1.5GB RAM)
# - phi3:small (HÄ±zlÄ± - 3GB RAM)

# OpenAI Configuration (Embedding iÃ§in)
OPENAI_API_KEY=your_openai_api_key_here

# QDRANT Vector Database Configuration
QDRANT_URL=http://localhost:6333
QDRANT_COLLECTION=ai_logs

# Server Configuration
PORT=3000
NODE_ENV=development

# Database Configuration
MONGODB_URI=mongodb://localhost:27017/ai_logs
```

### 6. **UygulamayÄ± BaÅŸlatÄ±n**

```bash
# Production
npm start

# Development (nodemon ile)
npm run dev
```

## ğŸ“Š API Endpoint'leri (Ã–zet)

-   **POST /api/query**: AI sorgusu gÃ¶nderme (vektÃ¶r veritabanÄ± ile geliÅŸtirilmiÅŸ, kullanÄ±cÄ± seÃ§enekleriyle)
-   **GET /api/history**: GeÃ§miÅŸ sorgularÄ± getirme
-   **POST /api/feedback**: YanÄ±t iÃ§in kullanÄ±cÄ± feedback'i kaydetme
-   **POST /api/mark-training**: Log'u eÄŸitim Ã¶rneÄŸi olarak iÅŸaretleme
-   **GET /api/training-examples**: EÄŸitim Ã¶rneklerini listeleme
-   **GET /api/vectors/list**: VektÃ¶r veritabanÄ± kayÄ±tlarÄ±nÄ± listeleme

## ğŸ”§ Ã–zellikler

-   Modern, responsive ve ÅŸeffaf chat arayÃ¼zÃ¼
-   GeliÅŸmiÅŸ RAG mimarisi (vektÃ¶r + keyword arama, Ã¶rnek Ã¶zetleme, prompt mÃ¼hendisliÄŸi)
-   KullanÄ±cÄ±ya yanÄ±t stili, formatÄ± ve uzunluÄŸu seÃ§me imkanÄ±
-   Her yanÄ±tÄ±n altÄ±nda kaynak gÃ¶sterimi ve otomatik deÄŸerlendirme
-   Feedback ve eÄŸitim verisi yÃ¶netimi
-   Kod, tablo ve madde iÅŸaretleri iÃ§in zengin format desteÄŸi
-   Llama 3.2 ve Phi-3 model desteÄŸi
-   Docker ve environment variable desteÄŸi

## ğŸ› ï¸ Teknolojiler

-   **Backend**: Node.js, Express.js
-   **VeritabanÄ±**: MongoDB, Mongoose
-   **VektÃ¶r VeritabanÄ±**: Qdrant
-   **AI Servisi**: Ollama (Llama 3.2, Phi-3)
-   **Embedding**: OpenAI Embeddings
-   **Frontend**: HTML, CSS, Vue.js
-   **Containerization**: Docker, Docker Compose

## ğŸ¯ SonuÃ§

Bu uygulama, modern AI chat deneyimi, geliÅŸmiÅŸ arama ve ÅŸeffaflÄ±k Ã¶zellikleriyle Ã¶ne Ã§Ä±kar. Hem teknik hem de kullanÄ±cÄ± deneyimi aÃ§Ä±sÄ±ndan gÃ¼ncel en iyi uygulamalarÄ± bir araya getirir.

# LLM Model SeÃ§imi ve KarÅŸÄ±laÅŸtÄ±rmasÄ±

## 1. Proje Ã–zeti

-   Node.js ve Express tabanlÄ±, haftalÄ±k Ã§alÄ±ÅŸma verilerini analiz eden bir AI asistanÄ±.
-   Qdrant vektÃ¶r veritabanÄ± ve MongoDB kullanÄ±yor.
-   Embedding iÅŸlemleri iÃ§in OpenAI API ve fallback olarak hash tabanlÄ± sistem mevcut.
-   AI servisinde model olarak Ollama ile Llama/phi3 kullanÄ±mÄ± entegre edildi.

## 2. KullanÄ±labilecek LLM Modelleri

### Llama (Meta)

-   AÃ§Ä±k kaynak, Ollama ile kolayca Ã§alÄ±ÅŸÄ±r.
-   Yerel olarak Ã§alÄ±ÅŸtÄ±rÄ±labilir, API anahtarÄ± gerekmez.
-   FarklÄ± boyutlarda modeller (3B, 7B, 70B) ile RAM ve hÄ±z ihtiyacÄ±na gÃ¶re seÃ§im yapÄ±labilir.
-   TÃ¼rkÃ§e desteÄŸi iyidir.
-   Maliyet yoktur, kota yoktur.
-   Veri gizliliÄŸi yÃ¼ksektir (veri dÄ±ÅŸarÄ± Ã§Ä±kmaz).

### Deepseek

-   AÃ§Ä±k kaynak, bazÄ± modeller Ollama ile Ã§alÄ±ÅŸabilir.
-   Kurulumu ve entegrasyonu Llama kadar kolay deÄŸildir.
-   TÃ¼rkÃ§e desteÄŸi Llama kadar gÃ¼Ã§lÃ¼ deÄŸildir.
-   Topluluk ve dÃ¶kÃ¼mantasyon desteÄŸi daha zayÄ±f.

### Claude (Anthropic)

-   Sadece bulut tabanlÄ± API ile kullanÄ±labilir.
-   API anahtarÄ± ve Ã¼cret gerektirir, kota sÄ±nÄ±rÄ± vardÄ±r.
-   TÃ¼rkÃ§e desteÄŸi iyidir, ancak Ã¼cretsiz ve yerel Ã§alÄ±ÅŸmaz.
-   Veri dÄ±ÅŸarÄ±ya Ã§Ä±kar, gizlilik daha dÃ¼ÅŸÃ¼ktÃ¼r.

## 3. Projeniz Ä°Ã§in En Uygun Model: Llama

-   Mevcut altyapÄ±nÄ±z Ollama ve Llama ile uyumlu.
-   API anahtarÄ± veya ek Ã¼cret gerektirmez.
-   RAM ve hÄ±z ihtiyacÄ±nÄ±za gÃ¶re model seÃ§ebilirsiniz.
-   Yerel Ã§alÄ±ÅŸtÄ±ÄŸÄ± iÃ§in veri gizliliÄŸi saÄŸlar.
-   AÃ§Ä±k kaynak ve topluluk desteÄŸi gÃ¼Ã§lÃ¼dÃ¼r.
-   TÃ¼rkÃ§e performansÄ± yÃ¼ksektir.

## 4. Llama Model Kurulumu ve KullanÄ±mÄ±

-   Ollama kurulu olmalÄ±.
-   Terminalden model indirmek iÃ§in:
    -   `ollama pull llama3.2:3b` (HÄ±zlÄ±, dÃ¼ÅŸÃ¼k RAM)
    -   `ollama pull llama3.2:7b` (Dengeli, Ã¶nerilen)
    -   `ollama pull llama3.2:70b` (YÃ¼ksek kalite, Ã§ok RAM)
-   .env dosyasÄ±na model adÄ±nÄ± yazÄ±n:
    -   `OLLAMA_MODEL=llama3.2:7b`
-   Web arayÃ¼zÃ¼nden de model seÃ§imi yapÄ±labilir.

## 5. RAM Gereksinimleri

-   3B model iÃ§in minimum 2GB RAM
-   7B model iÃ§in Ã¶nerilen 8GB RAM
-   70B model iÃ§in 40GB+ RAM

## 6. SonuÃ§ ve Ã–neri

-   Llama modelleri, projeniz iÃ§in en uygun, hÄ±zlÄ±, gÃ¼venli ve maliyetsiz Ã§Ã¶zÃ¼mdÃ¼r.
-   Deepseek ve Claude gibi alternatifler, ya daha karmaÅŸÄ±k ya da Ã¼cretli ve dÄ±ÅŸa baÄŸÄ±mlÄ±dÄ±r.
-   Llama ile hem yerel hem de esnek bir AI asistanÄ± altyapÄ±sÄ± kurabilirsiniz.
