# AI App Argenova

Bu proje, haftalÄ±k Ã§alÄ±ÅŸma verilerini analiz eden bir AI asistan uygulamasÄ±dÄ±r. Qdrant vektÃ¶r veritabanÄ± kullanarak geÃ§miÅŸ sorgularÄ± analiz eder ve daha eÄŸitilmiÅŸ yanÄ±tlar Ã¼retir.

## ğŸš€ **Yeni Ã–zellik: Llama Model DesteÄŸi**

Proje artÄ±k **Llama 3.2** modellerini desteklemektedir. KullanÄ±cÄ±lar farklÄ± Llama modelleri arasÄ±nda geÃ§iÅŸ yapabilir:

### ğŸ¤– **Desteklenen Modeller**

| Model             | Boyut | RAM   | HÄ±z    | Kalite | Ã–nerilen KullanÄ±m        |
| ----------------- | ----- | ----- | ------ | ------ | ------------------------ |
| **Llama 3.2 3B**  | 3B    | 2GB   | âš¡âš¡âš¡ | ğŸŸ¡     | HÄ±zlÄ± testler, dÃ¼ÅŸÃ¼k RAM |
| **Llama 3.2 7B**  | 7B    | 4GB   | âš¡âš¡   | ğŸŸ¢     | **VarsayÄ±lan - Dengeli** |
| **Llama 3.2 70B** | 70B   | 40GB  | âš¡     | ğŸ”´     | En yÃ¼ksek kalite         |
| **Phi-3 Mini**    | 3.8B  | 1.5GB | âš¡âš¡âš¡ | ğŸŸ¡     | Ã‡ok hÄ±zlÄ±                |
| **Phi-3 Small**   | 7B    | 3GB   | âš¡âš¡   | ğŸŸ¢     | HÄ±zlÄ± ve kaliteli        |

### ğŸ“¥ **Model Kurulumu**

```bash
# Ollama'ya model indirme
ollama pull llama3.2:3b    # HÄ±zlÄ± model
ollama pull llama3.2:7b    # VarsayÄ±lan model
ollama pull llama3.2:70b   # YÃ¼ksek kalite (40GB RAM gerekli)
ollama pull phi3:mini      # Ã‡ok hÄ±zlÄ±
ollama pull phi3:small     # HÄ±zlÄ± ve kaliteli

# Mevcut modelleri listele
ollama list

# Model bilgilerini gÃ¶rÃ¼ntÃ¼le
ollama show llama3.2:7b
```

### ğŸ›ï¸ **Model SeÃ§imi**

1. **Web ArayÃ¼zÃ¼**: Header'daki dropdown'dan model seÃ§in
2. **Environment Variable**: `.env` dosyasÄ±nda `OLLAMA_MODEL=llama3.2:7b`
3. **Otomatik Kaydetme**: SeÃ§ilen model localStorage'da saklanÄ±r

## ğŸ—ï¸ Proje Mimarisi

Proje MVC (Model-View-Controller) mimarisine uygun olarak dÃ¼zenlenmiÅŸtir:

```
ai-app-argenova/
â”œâ”€â”€ config/           # YapÄ±landÄ±rma dosyalarÄ±
â”‚   â”œâ”€â”€ database.js   # MongoDB baÄŸlantÄ±sÄ±
â”‚   â”œâ”€â”€ ai.js         # AI servis yapÄ±landÄ±rmasÄ± (Llama destekli)
â”‚   â”œâ”€â”€ qdrant.js     # Qdrant vektÃ¶r veritabanÄ±
â”‚   â””â”€â”€ embedding.js  # OpenAI embedding servisi
â”œâ”€â”€ models/           # VeritabanÄ± modelleri
â”‚   â””â”€â”€ Log.js        # Log ÅŸemasÄ±
â”œâ”€â”€ controllers/      # Ä°ÅŸ mantÄ±ÄŸÄ±
â”‚   â””â”€â”€ aiController.js # AI iÅŸlemleri
â”œâ”€â”€ routes/           # API route'larÄ±
â”‚   â””â”€â”€ aiRoutes.js   # AI endpoint'leri (Model yÃ¶netimi dahil)
â”œâ”€â”€ middleware/       # Ara yazÄ±lÄ±mlar
â”‚   â”œâ”€â”€ errorHandler.js    # Hata yÃ¶netimi
â”‚   â””â”€â”€ requestLogger.js   # Ä°stek loglama
â”œâ”€â”€ public/           # Statik dosyalar
â”‚   â”œâ”€â”€ index.html    # Model seÃ§ici UI
â”‚   â”œâ”€â”€ style.css     # Responsive tasarÄ±m
â”‚   â””â”€â”€ script.js     # Model yÃ¶netimi JS
â”œâ”€â”€ server.js         # Ana sunucu dosyasÄ±
â”œâ”€â”€ package.json
â”œâ”€â”€ docker-compose.yml # Docker servisleri
â””â”€â”€ .env.example      # Environment variables Ã¶rneÄŸi
```

## ğŸš€ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

### 1. **Ollama Kurulumu**

```bash
# Ubuntu/Debian
curl -fsSL https://ollama.ai/install.sh | sh

# Ollama servisini baÅŸlat
ollama serve

# Ä°lk modeli indir
ollama pull llama3.2:7b
```

### 2. **Docker Servislerini BaÅŸlatÄ±n**

```bash
docker-compose up -d
```

### 3. **BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¼kleyin**

```bash
npm install
```

### 4. **Environment Variables DosyasÄ±nÄ± OluÅŸturun**

```bash
cp .env.example .env
```

### 5. **.env DosyasÄ±nÄ± DÃ¼zenleyin**

```env
# AI Service Configuration
AI_SERVICE_URL=http://localhost:11434/api
AI_TEMPERATURE=0.7
AI_MAX_TOKENS=512

# Ollama Model Configuration
OLLAMA_MODEL=llama3.2:7b
# Model seÃ§enekleri:
# - llama3.2:3b (HÄ±zlÄ±, hafif - 2GB RAM)
# - llama3.2:7b (Dengeli - 4GB RAM)
# - llama3.2:70b (En yÃ¼ksek kalite - 40GB RAM)
# - phi3:mini (Ã‡ok hÄ±zlÄ± - 1.5GB RAM)
# - phi3:small (HÄ±zlÄ± - 3GB RAM)

# OpenAI Configuration (Embedding iÃ§in)
OPENAI_API_KEY=your_openai_api_key_here

# Qdrant Vector Database Configuration
QDRANT_URL=http://localhost:6333
QDRANT_COLLECTION=ai_logs

# Server Configuration
PORT=3000
NODE_ENV=development

# Database Configuration
MONGODB_URI=mongodb://localhost:27017/ai_logs
```

### 6. **UygulamayÄ± BaÅŸlatÄ±n**

```bash
# Production
npm start

# Development (nodemon ile)
npm run dev
```

## ğŸ“Š API Endpoint'leri

### POST /api/query

AI sorgusu gÃ¶nderme (vektÃ¶r veritabanÄ± ile geliÅŸtirilmiÅŸ)

```json
{
    "prompt": "HaftalÄ±k Ã§alÄ±ÅŸma verileriniz..."
}
```

### GET /api/history

GeÃ§miÅŸ sorgularÄ± getirme

```
GET /api/history?limit=10&page=1
```

### GET /api/models

Mevcut modelleri listeleme

```
GET /api/models
```

### GET /api/models/info/:modelName

Model bilgilerini getirme

```
GET /api/models/info/llama3.2:7b
```

### POST /api/populate-vectors

GeÃ§miÅŸ verileri vektÃ¶r veritabanÄ±na aktarma

```
POST /api/populate-vectors
```

## ğŸ”§ Environment Variables

| Variable            | AÃ§Ä±klama                        | VarsayÄ±lan                          |
| ------------------- | ------------------------------- | ----------------------------------- |
| `AI_SERVICE_URL`    | AI servis URL'si                | `http://localhost:11434/api`        |
| `AI_TEMPERATURE`    | AI yanÄ±t sÄ±caklÄ±ÄŸÄ±              | `0.7`                               |
| `AI_MAX_TOKENS`     | Maksimum token sayÄ±sÄ±           | `512`                               |
| `OLLAMA_MODEL`      | Ollama model adÄ±                | `llama3.2:7b`                       |
| `OPENAI_API_KEY`    | OpenAI API anahtarÄ±             | -                                   |
| `QDRANT_URL`        | Qdrant vektÃ¶r veritabanÄ± URL'si | `http://localhost:6333`             |
| `QDRANT_COLLECTION` | Qdrant koleksiyon adÄ±           | `ai_logs`                           |
| `PORT`              | Sunucu portu                    | `3000`                              |
| `NODE_ENV`          | Ã‡alÄ±ÅŸma ortamÄ±                  | `development`                       |
| `MONGODB_URI`       | MongoDB baÄŸlantÄ± URL'si         | `mongodb://localhost:27017/ai_logs` |

## ğŸ”§ Ã–zellikler

-   âœ… MVC mimarisi
-   âœ… Hata yÃ¶netimi
-   âœ… Ä°stek loglama
-   âœ… Pagination desteÄŸi
-   âœ… Graceful shutdown
-   âœ… Environment variable desteÄŸi
-   âœ… Input validation
-   âœ… Error handling middleware
-   âœ… Modern responsive UI
-   âœ… AI service configuration
-   âœ… **Qdrant vektÃ¶r veritabanÄ± entegrasyonu**
-   âœ… **OpenAI embedding servisi**
-   âœ… **Benzer sorgu analizi**
-   âœ… **GeliÅŸtirilmiÅŸ prompt oluÅŸturma**
-   âœ… **Docker Compose desteÄŸi**
-   âœ… **Llama 3.2 model desteÄŸi**
-   âœ… **Model seÃ§ici UI**
-   âœ… **Otomatik model kaydetme**

## ğŸ› ï¸ Teknolojiler

-   **Backend**: Node.js, Express.js
-   **VeritabanÄ±**: MongoDB, Mongoose
-   **VektÃ¶r VeritabanÄ±**: Qdrant
-   **AI Servisi**: Ollama (Llama 3.2, Phi-3)
-   **Embedding**: OpenAI Embeddings
-   **Frontend**: HTML, CSS, JavaScript
-   **Environment**: dotenv
-   **Containerization**: Docker, Docker Compose

## ğŸ” VektÃ¶r VeritabanÄ± Ã–zellikleri

### NasÄ±l Ã‡alÄ±ÅŸÄ±r?

1. **Embedding OluÅŸturma**: Her sorgu ve yanÄ±t OpenAI embedding API'si ile vektÃ¶re Ã§evrilir
2. **Benzerlik Arama**: Yeni sorgu geldiÄŸinde, geÃ§miÅŸ benzer sorgular bulunur
3. **Context OluÅŸturma**: Benzer Ã¶rnekler kullanÄ±larak geliÅŸtirilmiÅŸ prompt oluÅŸturulur
4. **EÄŸitilmiÅŸ YanÄ±t**: AI servisi daha zengin context ile yanÄ±t Ã¼retir

### Avantajlar

-   **Daha Kaliteli YanÄ±tlar**: GeÃ§miÅŸ Ã¶rneklerden Ã¶ÄŸrenme
-   **TutarlÄ±lÄ±k**: Benzer sorgulara benzer yanÄ±tlar
-   **SÃ¼rekli Ä°yileÅŸme**: Her yeni sorgu sistemi geliÅŸtirir
-   **HÄ±zlÄ± Arama**: VektÃ¶r benzerlik aramasÄ±

## ğŸ“ GeliÅŸtirme

Proje modÃ¼ler yapÄ±da tasarlanmÄ±ÅŸtÄ±r. Yeni Ã¶zellikler eklemek iÃ§in:

1. Model oluÅŸturun (`models/`)
2. Controller ekleyin (`controllers/`)
3. Route tanÄ±mlayÄ±n (`routes/`)
4. Gerekirse middleware ekleyin (`middleware/`)

## ğŸ”’ GÃ¼venlik

-   Environment variables kullanarak hassas bilgileri koruyun
-   Production ortamÄ±nda gÃ¼venli MongoDB URI kullanÄ±n
-   AI servis URL'sini environment variable'da saklayÄ±n
-   OpenAI API anahtarÄ±nÄ± gÃ¼venli ÅŸekilde saklayÄ±n

## ğŸ³ Docker KullanÄ±mÄ±

### Servisleri BaÅŸlatma

```bash
docker-compose up -d
```

### Servisleri Durdurma

```bash
docker-compose down
```

### LoglarÄ± GÃ¶rÃ¼ntÃ¼leme

```bash
docker-compose logs -f
```

## ğŸš€ Performans Optimizasyonu

### Model SeÃ§imi Rehberi

1. **GeliÅŸtirme/Test**: `llama3.2:3b` veya `phi3:mini`
2. **Production (4GB RAM)**: `llama3.2:7b`
3. **YÃ¼ksek Kalite (40GB RAM)**: `llama3.2:70b`
4. **HÄ±zlÄ± Ä°ÅŸlem**: `phi3:small`

### RAM Gereksinimleri

-   **Minimum**: 2GB (3B model iÃ§in)
-   **Ã–nerilen**: 8GB (7B model + sistem)
-   **YÃ¼ksek Kalite**: 40GB+ (70B model iÃ§in)

## ğŸ¯ SonuÃ§

**Llama 3.2** modelleri projeniz iÃ§in en uygun seÃ§enektir Ã§Ã¼nkÃ¼:

âœ… **Mevcut altyapÄ±nÄ±zla uyumlu** (Ollama)  
âœ… **Yerel Ã§alÄ±ÅŸma** (API anahtarÄ± gerektirmez)  
âœ… **Maliyet etkin** (Ã¼cretsiz)  
âœ… **Ã–lÃ§eklenebilir** (farklÄ± model boyutlarÄ±)  
âœ… **GÃ¼venli** (veri gizliliÄŸi)  
âœ… **HÄ±zlÄ±** (yerel iÅŸlem)

Projeniz artÄ±k tam iÅŸlevsel, modern ve Llama destekli bir AI asistan uygulamasÄ±dÄ±r! ğŸ‰

# LLM Model SeÃ§imi ve KarÅŸÄ±laÅŸtÄ±rmasÄ±

## 1. Proje Ã–zeti

-   Node.js ve Express tabanlÄ±, haftalÄ±k Ã§alÄ±ÅŸma verilerini analiz eden bir AI asistanÄ±.
-   Qdrant vektÃ¶r veritabanÄ± ve MongoDB kullanÄ±yor.
-   Embedding iÅŸlemleri iÃ§in OpenAI API ve fallback olarak hash tabanlÄ± sistem mevcut.
-   AI servisinde model olarak Ollama ile Llama/phi3 kullanÄ±mÄ± entegre edildi.

## 2. KullanÄ±labilecek LLM Modelleri

### Llama (Meta)

-   AÃ§Ä±k kaynak, Ollama ile kolayca Ã§alÄ±ÅŸÄ±r.
-   Yerel olarak Ã§alÄ±ÅŸtÄ±rÄ±labilir, API anahtarÄ± gerekmez.
-   FarklÄ± boyutlarda modeller (3B, 7B, 70B) ile RAM ve hÄ±z ihtiyacÄ±na gÃ¶re seÃ§im yapÄ±labilir.
-   TÃ¼rkÃ§e desteÄŸi iyidir.
-   Maliyet yoktur, kota yoktur.
-   Veri gizliliÄŸi yÃ¼ksektir (veri dÄ±ÅŸarÄ± Ã§Ä±kmaz).

### Deepseek

-   AÃ§Ä±k kaynak, bazÄ± modeller Ollama ile Ã§alÄ±ÅŸabilir.
-   Kurulumu ve entegrasyonu Llama kadar kolay deÄŸildir.
-   TÃ¼rkÃ§e desteÄŸi Llama kadar gÃ¼Ã§lÃ¼ deÄŸildir.
-   Topluluk ve dÃ¶kÃ¼mantasyon desteÄŸi daha zayÄ±f.

### Claude (Anthropic)

-   Sadece bulut tabanlÄ± API ile kullanÄ±labilir.
-   API anahtarÄ± ve Ã¼cret gerektirir, kota sÄ±nÄ±rÄ± vardÄ±r.
-   TÃ¼rkÃ§e desteÄŸi iyidir, ancak Ã¼cretsiz ve yerel Ã§alÄ±ÅŸmaz.
-   Veri dÄ±ÅŸarÄ±ya Ã§Ä±kar, gizlilik daha dÃ¼ÅŸÃ¼ktÃ¼r.

## 3. Projeniz Ä°Ã§in En Uygun Model: Llama

-   Mevcut altyapÄ±nÄ±z Ollama ve Llama ile uyumlu.
-   API anahtarÄ± veya ek Ã¼cret gerektirmez.
-   RAM ve hÄ±z ihtiyacÄ±nÄ±za gÃ¶re model seÃ§ebilirsiniz.
-   Yerel Ã§alÄ±ÅŸtÄ±ÄŸÄ± iÃ§in veri gizliliÄŸi saÄŸlar.
-   AÃ§Ä±k kaynak ve topluluk desteÄŸi gÃ¼Ã§lÃ¼dÃ¼r.
-   TÃ¼rkÃ§e performansÄ± yÃ¼ksektir.

## 4. Llama Model Kurulumu ve KullanÄ±mÄ±

-   Ollama kurulu olmalÄ±.
-   Terminalden model indirmek iÃ§in:
    -   `ollama pull llama3.2:3b` (HÄ±zlÄ±, dÃ¼ÅŸÃ¼k RAM)
    -   `ollama pull llama3.2:7b` (Dengeli, Ã¶nerilen)
    -   `ollama pull llama3.2:70b` (YÃ¼ksek kalite, Ã§ok RAM)
-   .env dosyasÄ±na model adÄ±nÄ± yazÄ±n:
    -   `OLLAMA_MODEL=llama3.2:7b`
-   Web arayÃ¼zÃ¼nden de model seÃ§imi yapÄ±labilir.

## 5. RAM Gereksinimleri

-   3B model iÃ§in minimum 2GB RAM
-   7B model iÃ§in Ã¶nerilen 8GB RAM
-   70B model iÃ§in 40GB+ RAM

## 6. SonuÃ§ ve Ã–neri

-   Llama modelleri, projeniz iÃ§in en uygun, hÄ±zlÄ±, gÃ¼venli ve maliyetsiz Ã§Ã¶zÃ¼mdÃ¼r.
-   Deepseek ve Claude gibi alternatifler, ya daha karmaÅŸÄ±k ya da Ã¼cretli ve dÄ±ÅŸa baÄŸÄ±mlÄ±dÄ±r.
-   Llama ile hem yerel hem de esnek bir AI asistanÄ± altyapÄ±sÄ± kurabilirsiniz.
